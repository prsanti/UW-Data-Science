{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Module 1: Introduction to Data Science - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "This module consists of two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "- **Part 1** - Setup and Installation Instructions\n",
    "- **Part 2** - Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "This module part provides a general overview of what constitutes Data Science, and the trends affecting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Outcomes\n",
    "\n",
    "In this module, you will learn the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The history of Data Science and its transition from traditional analytics\n",
    "* The difference between data analyst and scientist roles\n",
    "* The major technology shifts driving modern Data Science\n",
    "* Current Data Science applications and use cases in industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings and Resources\n",
    "\n",
    "There are no recommended readings and resources for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Module-1:-Introduction-to-Data-Science---Part-2\" data-toc-modified-id=\"Module-1:-Introduction-to-Data-Science---Part-2\">Module 1: Introduction to Data Science - Part 2</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction\">Introduction</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes\">Learning Outcomes</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Readings-and-Resources\" data-toc-modified-id=\"Readings-and-Resources\">Readings and Resources</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Table-of-Contents\" data-toc-modified-id=\"Table-of-Contents\">Table of Contents</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Introduction-to-Data-Science\" data-toc-modified-id=\"Introduction-to-Data-Science\">Introduction to Data Science</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#A-Brief-History-of-Data-Science\" data-toc-modified-id=\"A-Brief-History-of-Data-Science\">A Brief History of Data Science</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#The-Evolution-of-Analytics\" data-toc-modified-id=\"The-Evolution-of-Analytics\">The Evolution of Analytics</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Analytics-1.0:-Traditional-Analytics-(up-to-mid-2000s)\" data-toc-modified-id=\"Analytics-1.0:-Traditional-Analytics-(up-to-mid-2000s)\">Analytics 1.0: Traditional Analytics (up to mid-2000s)</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Analytics-2.0:-Big-Data-(mid-2000s)\" data-toc-modified-id=\"Analytics-2.0:-Big-Data-(mid-2000s)\">Analytics 2.0: Big Data (mid-2000s)</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Analytics-3.0:-Prescriptive-Analytics-(emerging-now)\" data-toc-modified-id=\"Analytics-3.0:-Prescriptive-Analytics-(emerging-now)\">Analytics 3.0: Prescriptive Analytics (emerging now)</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#From-Data-Analysts-to-Data-Scientists\" data-toc-modified-id=\"From-Data-Analysts-to-Data-Scientists\">From Data Analysts to Data Scientists</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Traditional-Analysts\" data-toc-modified-id=\"Traditional-Analysts\">Traditional Analysts</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Data-Scientists\" data-toc-modified-id=\"Data-Scientists\">Data Scientists</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Data-Science-is-Multidisciplinary\" data-toc-modified-id=\"Data-Science-is-Multidisciplinary\">Data Science is Multidisciplinary</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Big-Data\" data-toc-modified-id=\"Big-Data\">Big Data</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Predictive-Modeling\" data-toc-modified-id=\"Predictive-Modeling\">Predictive Modeling</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#The-Predictive-Modeling-Process\" data-toc-modified-id=\"The-Predictive-Modeling-Process\">The Predictive Modeling Process</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Data-Mining\" data-toc-modified-id=\"Data-Mining\">Data Mining</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#The-Analytics-Pipeline\" data-toc-modified-id=\"The-Analytics-Pipeline\">The Analytics Pipeline</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Machine-Learning\" data-toc-modified-id=\"Machine-Learning\">Machine Learning</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Artificial-Intelligence\" data-toc-modified-id=\"Artificial-Intelligence\">Artificial Intelligence</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Applications-of-Predictive-Modeling\" data-toc-modified-id=\"Applications-of-Predictive-Modeling\">Applications of Predictive Modeling</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Applications-in-Retail\" data-toc-modified-id=\"Applications-in-Retail\">Applications in Retail</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Segmentation\" data-toc-modified-id=\"Segmentation\">Segmentation</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Recommenders\" data-toc-modified-id=\"Recommenders\">Recommenders</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Market-Basket-Analysis\" data-toc-modified-id=\"Market-Basket-Analysis\">Market Basket Analysis</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Churn-Prevention\" data-toc-modified-id=\"Churn-Prevention\">Churn Prevention</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Applications-in-Financial-Services\" data-toc-modified-id=\"Applications-in-Financial-Services\">Applications in Financial Services</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Fraud-Detection\" data-toc-modified-id=\"Fraud-Detection\">Fraud Detection</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Financial-Markets\" data-toc-modified-id=\"Financial-Markets\">Financial Markets</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Applications-in-Healthcare\" data-toc-modified-id=\"Applications-in-Healthcare\">Applications in Healthcare</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Diagnosis\" data-toc-modified-id=\"Diagnosis\">Diagnosis</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Drug-Discovery\" data-toc-modified-id=\"Drug-Discovery\">Drug Discovery</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#In-the-Coming-Weeks\" data-toc-modified-id=\"In-the-Coming-Weeks\">In the Coming Weeks</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#References\" data-toc-modified-id=\"References\">References</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Data Science” is a fairly new term for a profession that applies the scientific method to analysis of data, and in particular, Big Data. Collecting, storing, and making sense of Big Data (another fairly new term) is quickly becoming part of every business and everyone’s life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief History of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a few of the key trends and milestones that led us to modern Data Science:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Year(s)*** | ***Trends*** |\n",
    "| ---: | :--- |\n",
    "| **1960s-1970s** | Rapid advances in statistics and computer science |\n",
    "| **Late 1990s** | Google invented a new search engine combining math, statistics, data engineering and computation |\n",
    "| **2000s** | Hard drives and computational power (particularly floating point computation using graphics cards) continued to become less expensive |\n",
    "| **2001** | The term *Data Science* was coined|\n",
    "| **2002** | CODATA Data Science journal launched |\n",
    "| **2003** | Columbia University began publishing *The Journal of Data Science*. Google File System paper spawns *Hadoop* |\n",
    "| **2006** | Moore's Law came to an end resulting in a shift towards parallel processing on multiple CPUs |\n",
    "| **2010s** | Rapid development of Deep Learning (Deep Learning, nd) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Evolution of Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics 1.0: Traditional Analytics (up to mid-2000s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional Analytics was primarily reactive:\n",
    "\n",
    "- Primarily descriptive and focused on reporting\n",
    "- Internally sourced, relatively small, structured data\n",
    "- \"Backroom\" teams of analysts\n",
    "- Internal Systems of Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics 2.0: Big Data (mid-2000s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern Analytics involves sophisticated modelling and in some cases requires complex data streaming and processing infrastructure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complex, large, unstructured data sources\n",
    "- Stored and processed rapidly, with new analytical and computational technologies\n",
    "- \"Data Scientists\" emerge\n",
    "- Online firms create data-based products and services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics 3.0: Prescriptive Analytics (emerging now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are entering a new era in which predictive models will guide business leaders in real-time decision-making:\n",
    "- An environment which combines Analytics 1.0 and 2.0 that yields insights with speed and impact\n",
    "- Analytics is integral to running a business and becomes part of strategy and operations\n",
    "- Predictive and Prescriptive Models: models that recommend specific actions\n",
    "- Artificial Intelligence techniques such as Deep Learning and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Data Analysts to Data Scientists\n",
    "The role of the data specialist has been evolving as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Analysts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional data analysts can be found in many corporate departments, and in particular Finance, Marketing and IT (Business Intelligence). They tend to be reactive, responding to specific requests for information or developing and overseeing regular reporting.  They typically use proprietary (i.e. not open source) tools such as *Excel*, *SAS*, *SPSS*, *Cognos*, etc. and have strong *SQL* database query skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern data scientists have a preference for open source tools such as *Python* and *R*, in addition to traditional toolsets.  For large volumes of data they typically use a Big Data analytics system such as *Spark*. Once you learn the foundations of Data Science and gain hands-on experience with some of these tools you can easily transition to others. The underlying skills are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science is Multidisciplinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"dataScienceOverlap.png\" alt=\"Data Science is oriented at the intersection of a large number of traditional disciplines. Currently, we recognize that Data Scientists encompass expertise from the fields of mathematics, statistics, machine learning and data mining, business, software engineering, data engineering, programming, visualization, storytelling and subject area expertise.\" style=\"width: 62%;\">\n",
    "    <figcaption><em>Data Science is oriented at the intersection of a large number of traditional disciplines. Currently, we recognize that Data Scientists encompass expertise from the fields of mathematics, statistics, machine learning and data mining, business, software engineering, data engineering, programming, visualization, storytelling and subject area expertise. (Course Authors, 2018)</em></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science draws on a wide variety of disciplines, skills and bodies of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Discipline/Skill/Knowledge*** | ***Reasoning*** |\n",
    "| ---: | :--- |\n",
    "| **Mathematics** | Data Science uses mathematical techniques to model phenomena in the world and business to gain useful insights and couple them with a measure of confidence in any conclusions drawn |\n",
    "| **Statistics** | Statistical procedures enable conclusions to made or insights to be taken from large volumes of data by looking for correlations (relationships) between variables |\n",
    "| **Machine Learning and Data Mining** | Techniques for automated detection of correlations are the \"core technology\" of data science |\n",
    "| **Business** | In a business setting, all of the usual business-related skills are essential. *(i.e., the ability to identify tradeoffs between investment in data science staff and tools vs. likely payoffs, working with other departments to identify potential data science projects and quantifying their value, negotiating for availability of resources, etc.)* |\n",
    "| **Software Engineering** | Often predictive models developed by a corporate data science team will be deployed into the data processing infrastructure of the company, which means they must consider tradeoffs such as throughput and latency in addition to predictive power |\n",
    "| **Data Engineering** | When dealing with \"Big Data\" (defined below), new scalable data streaming and distributed database technologies come into play, along with new issues and strategies for dealing with them |\n",
    "| **Programming** | Although the machine learning part of data science is mostly done using tools that have been developed by specialists, there is a lot of work to get a dataset prepared for analysis, and this often requires writing custom scripts to clean up the data |\n",
    "| **Visualization** | Often relationships in data are easier to see in pictures than numbers and can be much more convincing |\n",
    "| **Story Telling** | A data scientist must be able to tell a story about how their conclusions fit together into the bigger picture, especially if some kind of action must be taken by others as a result |\n",
    "| **Subject Area Expertise** | Data science is a tool in service of business, social goals or other branches of science; all of your existing skills and knowledge will help you identify worthwhile projects to turn the data science lens towards |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientists are rarely deep experts in more than one or two of these areas but must have competence in at least basic statistics and a working knowledge of most other areas.  Math and statistics are core skills but a pure degree in statistics isn't usually necessary.  Most data scientists come from a *STEM (Science, Technology, Engineering and Math)* background but artists with expertise in visualization and storytelling are equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "*Big Data* is a relatively new term &mdash; however collecting, storing and analyzing data is centuries old. The concept gained momentum in the early 2000s when industry analyst *Doug Laney* articulated the now-mainstream definition of Big Data as the three V's: *Velocity, Variety and Volume* (Downey, 2015).\n",
    "\n",
    "Data becomes \"Big\" when the rate at which it arrives, the variety of data formats you need to deal with, and/or the quantities to be processed grow to a level at which a single-server processing solution will no longer be adequate.\n",
    "\n",
    "Relational databases can't be distributed across more than a few servers, and synchronization between them becomes near impossible when dealing with large or globally distributed data sources.  Big Data inevitably involves new technologies designed for distributed, eventual data consistency systems and so a new branch of *Data Science* known as *Data Engineering* has emerged.\n",
    "\n",
    "Others have extended Laney's three V's in various ways, for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"bigDataVs.png\" alt=\"This image shows the extensions of Laney's V's as a diagram.\" style=\"width: 38%;\">\n",
    "    <figcaption><em>This diagram shows extensions of Laney's three V. (Course Authors, 2018)</em></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally you may see references to another V, \"Veracity\" &mdash; which essentially means truthfulness &mdash; and can be considered to encompass correctness and accuracy.\n",
    "\n",
    "In this diagram we see the original three V's enhanced with another three key characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Additional Characteristics*** | ***Reasoning*** |\n",
    "| ---: | :--- |\n",
    "| **Variability** | In the online world, system usage can go from minimal to massive demand in a matter of seconds and should be able scale up and down quickly in response |\n",
    "| **Complexity** | The various kinds of data available and the variety of ways it can be encoded with descriptive tags has exploded |\n",
    "| **Value** | Does acquiring this data represent good value for the costs involved? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling\n",
    "\n",
    "Predictive Modelling is a process of creating a model that, given a set of inputs, will produce a plausible output similar to some process it is meant to mimic.\n",
    "Predictive models are usually statistical in nature. We don't usually know the precise mapping of inputs to outputs or  what all of the input variables are &mdash; much less their current values.\n",
    "So we can't define a precise mathematical function but there may be significant value in having a good approximation.\n",
    "\n",
    "The development of the discipline of Machine Learning gives us tools for discovering such approximations by analyzing the patterns in large datasets.\n",
    "With a large enough dataset of examples of real inputs and outputs these tools can learn a decent input-to-output mapping.\n",
    "\n",
    "For example:\n",
    "- Which behaviours (input) are correlated with subsequent fraud (output) \n",
    "- Which photos (input) do or do not contain a picture of a goat (output)\n",
    "\n",
    "The rise of the internet has enabled collection of huge datasets that make this kind of analysis feasible and has led to recent advances in areas such as facial recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Predictive Modeling Process\n",
    "The process of developing a predictive model involves several steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"dataScienceProcess.png\" alt=\"This image shows the stages of creating a model from start to finish.\" style=\"width: 100%;\"> (Course Authors, 2018)\n",
    "    <figcaption><em>This image shows the stages of creating a model from start to finish.</em></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Steps*** | ***Description*** |\n",
    "| ---: | :--- |\n",
    "| **Define Problem** | Especially in a business context, the best results in terms of value-for-money come from tackling a *well-defined* problem. The solution to the problem may not be evident at first and some exploration in terms of data collection and analysis may be required, but it is best to have a specific objective in mind. |\n",
    "| **Prepare Data** | This involves finding the data you need (which could be from several sources), confirming its suitability, and transforming it into a format amenable to subsequent analysis. |\n",
    "| **Create Model** | This step involves an initial analysis of the dataset to find useful patterns and selecting one or more modelling approaches.  Data scientists will also need to confirm the toolset and which methods to use.  If machine learning techniques are used, this stage will also involve *training* the model on a dataset of previously-observed results for a given set of inputs for the process being modeled. |\n",
    "| **Test Model** | This phase involves confirming that the model is operating as expected. |\n",
    "| **Validate Model** | In this step, the model is presented with test cases of inputs and outputs similar to those used for training the model to confirm that it makes predictions that are similar.  As we will see later, we almost always want the model to *generalize* and not rote-learn an input-to-output mapping.  Validating with data that *was not used to train the model* will help avoid developing models that have not learned to generalize. |\n",
    "| **Evaluate Model** | Data scientists will often develop several models and compare their performance to identify the best.  Counterintuitively, sometimes the best model is a weighted average of several models (an *ensemble* of models) that individually are not the best performers. |\n",
    "| **Deploy Model** | In a business context, this involves integrating the model into existing business processes and utilizing newly received data to update the model's predictions to generate business value. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining\n",
    "Sometimes the focus is less on developing a predictive model and more on discovering relationships within a dataset that represent new and valuable insights.  The term Data Mining is used to refer to these scenarios.\n",
    "\n",
    "Some examples include: \n",
    "- Using a large dataset of network events captured from a credit card processing system and identifying patterns that represent attempts by hackers to break in\n",
    "- Identifying behaviours of smartphone customers indicating they're leaving for a competitor, so that the customer relationship team can proactively attempt to retain them\n",
    "\n",
    "These insights could also be used to formulate a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Analytics Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"dataCapture.png\" alt=\"This image shows the major stages of producing an analysis. (Course Authors, 2018)\" style=\"width: 62%;\">\n",
    "    <figcaption><em>This image shows the major stages of producing an analysis. (Course Authors, 2018)</em></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many data-intensive companies have now deployed an Analytics Pipeline similar to the one shown in the diagram.  The details vary from organization to organization, but generally follow this broad pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Pipeline Stage*** | ***Description*** |\n",
    "| ---: | :--- |\n",
    "| **Data Sources**  | Data arrives from a variety of systems and providers.  Increasingly, this includes the *Internet-of-Things (IoT)* &mdash; devices attached to the Internet (as opposed to people directly). Some examples are:<br> <ul><li>Cellphones generate a huge amount of streaming data about people's location, app usage, phone usage, etc.</li> <li>In auto insurance and fleet management, real-time information about car and truck drivers' behavior is increasingly being monitored</li> <li>Sensors in modern tractors continually monitor the soil conditions for farmers and are sent to agricultural product manufacturers who use the information to recommend improvements to drainage, pesticide and fertilizer use</li></ul>  <br>A second source of data is a company's internal systems: <br> <ul><li>Banks use internally-sourced data aggregated from across their many lines of business to create a consolidated risk management view</li> <li>Retailers scour sales data from their *point-of-sale* terminals to spot trends and behaviors they can use to drive profitability</li> <li>For large tech companies (*Google, LinkedIn, etc.*) their users are a massive source of real-time data about opinions, trends and behaviors at the individual level</li> <li>And, in our connected society, companies increasingly share data from outside their walls: with trading and supply chain partners, and by purchasing data from 3rd party data providers</li></ul> |\n",
    "| **Data Capture and Processing** | The arriving data needs to be captured and saved for subsequent analysis.  Organizations are increasingly capturing data in its raw form in a database optimized for fast data input.  This is a departure from the past where data was often carefully validated and preprocessed prior to storage in a near-normalized form in a relational database.  In some applications today (especially Web or IoT-based) the arriving data rates are so high that data capture using relational technology is infeasible.  Relatively new database technologies such as *Hadoop* were designed for this purpose: very fast writing of large volumes of data.  A database used this way, as a capture area for raw data is often called a *data landing area* or *data lake*.  <br><br>Another reason for storing raw data is that preprocessing assumes a particular use for the data and as a result throws away some of its content.  If the data is stored in its raw form, all of the information content is preserved in case additional forms of analysis might require it.  Once the data has landed, data scientists can use it to build models or conduct enquiries, often transforming the data to improve its quality or make it more amenable to analysis.  Transformations and models that prove to be useful over the long term can be automated and deployed as on-going processes that produce regular outputs. |\n",
    "| **Analysis Results** | The outputs of this overall process include ad-hoc and on-going analysis, predictions, standing reports, and insights into the data that may lead to business process improvements or better margins. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Throughout most of the twentieth century, the standard approach for building a predictive model started with identifying the features of the business, social or economic process of interest that were likely to have some predictive power.  In other words, we identified the independent variables (inputs) to the model in advance of building the model.  This approach is  known as *Feature Engineering* and is still important.  But we now have many tools to assist us in finding predictive variables empirically.\n",
    "\n",
    "Machine Learning is a term used to describe the automatic extraction of knowledge from data.  By knowledge, we mean a set of generalizations that can be made about the data.  These generalizations can be hard rules, statements that appear to always be true (at least within a given dataset), or soft rules that are approximately or probabilistically true.  Suppose, for example, you knew nothing about the rules of the road.  Analyzing a dataset of nothing but traffic patterns might allow you to infer that driving only one way on some streets is a hard rule (unless you have a dataset that's big enough to contain an example of someone doing it).  Speed limits would become apparent as a very soft rule (in fact, a distribution around a speed is no doubt a little above the posted limit).\n",
    "\n",
    "There are a variety of Machine Learning algorithms that have been developed for this purpose, each with their own strategy for extracting and representing knowledge: some as actual rules, some as tree structures, some as statistical distributions, often as combinations of these.  The process of extracting the knowledge from a dataset is called *training* or learning a model.\n",
    "\n",
    "Each algorithm has its own performance characteristics.  Some are fast to train but slow to compute a prediction whereas the opposite is true of others.  Some are more resilient to noise in the data than others.  Paradoxically, simple models often perform as well as highly sophisticated ones for many business applications.  We will encounter many of these algorithms in the courses in this series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence\n",
    "\n",
    "You may be wondering about the difference between Machine Learning and *Artificial Intelligence (AI)*.  Machine Learning is a particular set of knowledge and techniques within the much broader topic of AI.  \n",
    "\n",
    "* AI comprises the entire question of whether and how a machine can show behaviors that are sufficiently human-like to be indistinguishable from the real thing.  It includes general theories of knowledge representation and strategies for searching for solutions to complex problems in a large space of alternatives.  \n",
    "* Machine Learning is a subset of AI that is focused on extracting knowledge from observed data with little human guidance whereas AI more generally allows for incorporating generic or hand-crafted problem-solving strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Predictive Modeling\n",
    "Let's take a look at the myriad applications of Data Science and Predictive Modeling.  Here are a few industries we will focus on:\n",
    "- Retail\n",
    "- Financial Services\n",
    "- Healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications in Retail\n",
    "#### Segmentation\n",
    "Customer segmentation is a type of predictive model.  For example, we would like to predict which segment a customer is likely to fall into, so that we can tailor our marketing strategy to meet their needs and increase their satisfaction.\n",
    "\n",
    "Why segment markets? Customers may differ in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What they want to buy\n",
    "* Amount willing to pay\n",
    "* Quantity they buy\n",
    "* Time, place, and frequency of purchase\n",
    "* Personal taste (likes and dislikes), for example in:\n",
    "\n",
    "    * Media\n",
    "    * Phone plan\n",
    "    * Newspapers\n",
    "    * Magazines\n",
    "    * Movies\n",
    "    * Social media platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a retailer knows that you belong to a segment that shops often, and on average spends \\\\$100 per transaction, their strategy may be to try to \"stretch you\" to spend \\\\$125 on your next purchase in the hope that it will become a new pattern.  They could do this by offering you a coupon, or a certain promotion (spend \\\\$150 and get \\\\$25 back).\n",
    "\n",
    "Some customers may even represent negative value: ones that come in and scoop up advertised loss leaders but don't buy anything else while in the store.  A retailer may devise strategies to minimize traffic from this segment.\n",
    "\n",
    "#### Recommenders\n",
    "Online retailers in particular have developed sophisticated methods for recommending products to customers based on their online history, interests or similarity to other customers.  Online recommendation engines seek to match consumers behavior profiles with those of others and to recommend products that similar customers have bought in the past.\n",
    "\n",
    "#### Market Basket Analysis\n",
    "Market Basket Analysis is a technique where retailers look for correlations in items that consumers tend to buy at the same time.  This kind of analysis can be used for recommendations, but also as a way of maximizing profit.  Some items naturally go together, like burgers and condiments, so a grocery store could advertise a special on ground beef yet ensure all condiments are at full price that week.  Retailers continually mine stored data for more subtle associations between items and use the insights to inform their pricing strategy.\n",
    "\n",
    "#### Churn Prevention\n",
    "It's an old saying that it takes about ten times as much money to acquire a new customer as it does to retain an existing one.  Retailers and services providers such as Telcos are deeply interested in ways to boost brand loyalty or intervene meaningfully when a customer is considering switching to another brand. As part of churn prevention, some companies mine the data they have on the behavior of their customers or their comments on social media to determine if customers are considering competitors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications in Financial Services\n",
    "#### Fraud Detection\n",
    "Fraud detection is serious business in the online world.  Online marketplaces perform real-time fraud detection to decide whether to allow transactions to take place between parties.  Modern fraud detection systems consider a wide variety of factors such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The location of the parties to the transaction\n",
    "* Whether a user has a new account but is using the app's features like an experienced user\n",
    "* Whether an account is linked with other accounts in a fashion that would facilitate money laundering\n",
    "* With whom the parties have other relationships\n",
    "* Whether the names and addresses of the parties are credible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Financial Markets\n",
    "Attempting to \"beat the market\" is as old as exchanges and people continue to build increasingly more sophisticated predictive models to attempt to do so.  Some also incorporate other AI techniques to try to discover positive or negative impacts from electronic news releases and execute trades faster than people can react."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications in Healthcare\n",
    "#### Diagnosis\n",
    "\n",
    "Startups are developing diagnoses for various kinds of cancer by analyzing skin photos or breath samples, using predictive models trained to recognize telltale signs.  *Deep Learning*, a branch of Machine Learning that has revolutionized face and object recognition in photos, is now being used to analyze X-ray photos for signs of disease. \n",
    "\n",
    "\n",
    "#### Drug Discovery\n",
    "Startups are using Machine Learning to identify potential drug candidates by analyzing patterns in the chemical properties of previously successful and unsuccessful drugs.  Lab testing of compounds is a slow and costly process which can be dramatically accelerated by focusing the search on compounds most likely to have desirable properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the Coming Weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be covering a broad introduction to Data Science over the next few weeks.  Throughout the course (and overall program) we will be working in the *Python* programming language, which has become the number one choice for Data Science, slightly ahead of *R*.  We will begin by learning the basics of Python, then focus on the **Numpy** (Numpy developers, 2018), **Pandas** (Pandas community, 2018), and **Scikit-Learn** (Scikit-learn developers, 2018) libraries for Statistics and Machine Learning.  As you build your skills, you will use them to characterize datasets and begin building simple predictive models using these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__End of Module__\n",
    "\n",
    "You have reached the end of this module. \n",
    "\n",
    "If you have any questions, please reach out to your peers using the discussion boards. If you and your peers are unable to come to a suitable conclusion, do not hesitate to reach out to your instructor on the designated discussion board.\n",
    "\n",
    "When you are comfortable with the content, and have practiced to your satisfaction, you may proceed to any related assignments, and to the next module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Deep Learning, (nd). wikipedia, the free encyclopedia. Accessed Sept 4, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Deep_learning&oldid=850422779#History)\n",
    "\n",
    "Downey, A. (2015). Think python--how to think like a computer scientist. [online](http://greenteapress.com/wp/think-python-2e/)\n",
    "\n",
    "NumPy developers, 2018. Numpy. [online](http://www.numpy.org)\n",
    "\n",
    "Pandas community, 2018. Pandas. [online](http://pandas.pydata.org/index.html)\n",
    "\n",
    "Scikit-learn developers, 2018. Scikit-learn: machine learning in python — scikit-learn 0.19.2 documentation. [online](http://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
