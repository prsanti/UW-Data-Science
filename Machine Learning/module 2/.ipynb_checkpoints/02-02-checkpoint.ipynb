{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Module 2 Part 2: The Machine Learning Process, a Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true,
    "toc-hr-collapsed": false
   },
   "source": [
    "This module consists of 2 parts:\n",
    "\n",
    "- **Part 1** - The Machine Learning Process, a Worked Example (Data Exploration)\n",
    "\n",
    "- **Part 2** - The Machine Learning Process, a Worked Example (Data Modelling)\n",
    "\n",
    "Each part is provided in a separate notebook file. It is recommended that you follow the order of the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Module-2-Part-2:-The-Machine-Learning-Process,-a-Worked-Example\" data-toc-modified-id=\"Module-2-Part-2:-The-Machine-Learning-Process,-a-Worked-Example\">Module 2 Part 2: The Machine Learning Process, a Worked Example</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Table-of-Contents\" data-toc-modified-id=\"Table-of-Contents\">Table of Contents</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Data-Sources\" data-toc-modified-id=\"Data-Sources\">Data Sources</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Pre-processing-Pipelines\" data-toc-modified-id=\"Pre-processing-Pipelines\">Pre-processing Pipelines</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Imputers\" data-toc-modified-id=\"Imputers\">Imputers</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Categorical-encoders\" data-toc-modified-id=\"Categorical-encoders\">Categorical encoders</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Feature-scalers\" data-toc-modified-id=\"Feature-scalers\">Feature scalers</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Custom-transformers\" data-toc-modified-id=\"Custom-transformers\">Custom transformers</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Variable-buckets\" data-toc-modified-id=\"Variable-buckets\">Variable buckets</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Bringing-the-pipeline-together\" data-toc-modified-id=\"Bringing-the-pipeline-together\">Bringing the pipeline together</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Train-test-Split\" data-toc-modified-id=\"Train-test-Split\">Train-test Split</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Stratified-Sampling\" data-toc-modified-id=\"Stratified-Sampling\">Stratified Sampling</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Basic-Regressions\" data-toc-modified-id=\"Basic-Regressions\">Basic Regressions</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Advanced-Regressions\" data-toc-modified-id=\"Advanced-Regressions\">Advanced Regressions</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#ElasticNet\" data-toc-modified-id=\"ElasticNet\">ElasticNet</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#SGDRegressor\" data-toc-modified-id=\"SGDRegressor\">SGDRegressor</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#DecisionTreeRegressor\" data-toc-modified-id=\"DecisionTreeRegressor\">DecisionTreeRegressor</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#EXERCISE-4:-Building-a-regression-model\" data-toc-modified-id=\"EXERCISE-4:-Building-a-regression-model\">EXERCISE 4: Building a regression model</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation\">Cross-validation</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Hyperparameter-Search\" data-toc-modified-id=\"Hyperparameter-Search\">Hyperparameter Search</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Conclusion:-Launch,-Monitor,-Maintain\" data-toc-modified-id=\"Conclusion:-Launch,-Monitor,-Maintain\">Conclusion: Launch, Monitor, Maintain</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Launch\" data-toc-modified-id=\"Launch\">Launch</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Monitor\" data-toc-modified-id=\"Monitor\">Monitor</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Maintain\" data-toc-modified-id=\"Maintain\">Maintain</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Taking-this-further\" data-toc-modified-id=\"Taking-this-further\">Taking this further</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#Exercise-Solutions\" data-toc-modified-id=\"Exercise-Solutions\">Exercise Solutions</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#EXERCISE-4\" data-toc-modified-id=\"EXERCISE-4\">EXERCISE 4</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#References\" data-toc-modified-id=\"References\">References</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "\n",
    "You can find the original dataset [here](http://insideairbnb.com/get-the-data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a-b)**2))\n",
    "pd.options.display.max_columns = None\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Pipelines\n",
    "\n",
    "In Part 1 of the module, we focused on exploring our data. We identified features that require pre-processing before they can be used in a machine learning model. We covered imputation, log transformations, and category binning. In addition to these changes, we will need to transform the data to satisfy the mathematical assumptions of our models. These changes include scaling our numerical data and encoding our categorical data.\n",
    "\n",
    "The `scikit-learn` package has a [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that can be used to handle various pre-processing steps. In addition to the pipeline, `pandas` formatted data can be transformed using the [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). We will use these tools combined with multiple pre-processing techniques to transform the raw data into useable features for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we'll reload the dataset from the source file and separate the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/toronto_listings_119.csv', index_col='id')\n",
    "# a bit of extra pre-processing\n",
    "# others = [x for x in df['property_type'].value_counts().index if df['property_type'].value_counts()[x] <= 10]\n",
    "# df['property_type'] = df['property_type'].apply(lambda x: 'Other' if x in others else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the dataset. It has some missing values, lots of categorical variables, and will require some data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_listings_count</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>is_location_exact</th>\n",
       "      <th>property_type</th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>amenities</th>\n",
       "      <th>price</th>\n",
       "      <th>guests_included</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>review_scores_accuracy</th>\n",
       "      <th>review_scores_cleanliness</th>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>reviews_per_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30813194</th>\n",
       "      <td>f</td>\n",
       "      <td>2.0</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>43.760928</td>\n",
       "      <td>-79.411002</td>\n",
       "      <td>t</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Shared room</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>{Wifi,\"Air conditioning\",Elevator,Heating,Wash...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2</td>\n",
       "      <td>365</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>t</td>\n",
       "      <td>flexible</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823607</th>\n",
       "      <td>f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>43.657402</td>\n",
       "      <td>-79.455290</td>\n",
       "      <td>t</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>{TV,\"Cable TV\",Internet,Wifi,Kitchen,\"Free par...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>92.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>f</td>\n",
       "      <td>flexible</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12835392</th>\n",
       "      <td>f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>43.764857</td>\n",
       "      <td>-79.412793</td>\n",
       "      <td>t</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>{TV,Internet,Kitchen,\"Free parking on premises...</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>flexible</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28183298</th>\n",
       "      <td>f</td>\n",
       "      <td>2.0</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>43.685043</td>\n",
       "      <td>-79.399169</td>\n",
       "      <td>t</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Pull-out Sofa</td>\n",
       "      <td>{Internet,Wifi,\"Wheelchair accessible\",Kitchen...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>358</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>strict_14_with_grace_period</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19680294</th>\n",
       "      <td>f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>43.617793</td>\n",
       "      <td>-79.487189</td>\n",
       "      <td>f</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Airbed</td>\n",
       "      <td>{Wifi,Kitchen,\"Pets live on this property\",Was...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>t</td>\n",
       "      <td>flexible</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         host_is_superhost  host_listings_count host_has_profile_pic  \\\n",
       "id                                                                     \n",
       "30813194                 f                  2.0                    t   \n",
       "2823607                  f                  1.0                    t   \n",
       "12835392                 f                  1.0                    t   \n",
       "28183298                 f                  2.0                    t   \n",
       "19680294                 f                  1.0                    t   \n",
       "\n",
       "         host_identity_verified   latitude  longitude is_location_exact  \\\n",
       "id                                                                        \n",
       "30813194                      f  43.760928 -79.411002                 t   \n",
       "2823607                       t  43.657402 -79.455290                 t   \n",
       "12835392                      t  43.764857 -79.412793                 t   \n",
       "28183298                      t  43.685043 -79.399169                 t   \n",
       "19680294                      f  43.617793 -79.487189                 f   \n",
       "\n",
       "         property_type        room_type  accommodates  bathrooms  bedrooms  \\\n",
       "id                                                                           \n",
       "30813194     Apartment      Shared room             1        1.0       1.0   \n",
       "2823607      Apartment  Entire home/apt             3        1.0       1.0   \n",
       "12835392     Apartment  Entire home/apt             1        1.0       1.0   \n",
       "28183298     Apartment  Entire home/apt             1        1.0       0.0   \n",
       "19680294     Apartment     Private room             3        1.0       1.0   \n",
       "\n",
       "          beds       bed_type  \\\n",
       "id                              \n",
       "30813194   1.0       Real Bed   \n",
       "2823607    1.0       Real Bed   \n",
       "12835392   1.0       Real Bed   \n",
       "28183298   1.0  Pull-out Sofa   \n",
       "19680294   1.0         Airbed   \n",
       "\n",
       "                                                  amenities  price  \\\n",
       "id                                                                   \n",
       "30813194  {Wifi,\"Air conditioning\",Elevator,Heating,Wash...   48.0   \n",
       "2823607   {TV,\"Cable TV\",Internet,Wifi,Kitchen,\"Free par...  100.0   \n",
       "12835392  {TV,Internet,Kitchen,\"Free parking on premises...  145.0   \n",
       "28183298  {Internet,Wifi,\"Wheelchair accessible\",Kitchen...   80.0   \n",
       "19680294  {Wifi,Kitchen,\"Pets live on this property\",Was...   50.0   \n",
       "\n",
       "          guests_included  availability_365  number_of_reviews  \\\n",
       "id                                                               \n",
       "30813194                2               365                  2   \n",
       "2823607                 1                 0                 50   \n",
       "12835392                1                 0                  1   \n",
       "28183298                1               358                  0   \n",
       "19680294                1                 0                  8   \n",
       "\n",
       "          review_scores_rating  review_scores_accuracy  \\\n",
       "id                                                       \n",
       "30813194                 100.0                     9.0   \n",
       "2823607                   92.0                    10.0   \n",
       "12835392                   NaN                     NaN   \n",
       "28183298                   NaN                     NaN   \n",
       "19680294                  90.0                     9.0   \n",
       "\n",
       "          review_scores_cleanliness  review_scores_checkin  \\\n",
       "id                                                           \n",
       "30813194                        9.0                    9.0   \n",
       "2823607                         9.0                   10.0   \n",
       "12835392                        NaN                    NaN   \n",
       "28183298                        NaN                    NaN   \n",
       "19680294                        9.0                   10.0   \n",
       "\n",
       "          review_scores_communication  review_scores_location  \\\n",
       "id                                                              \n",
       "30813194                          9.0                     9.0   \n",
       "2823607                          10.0                     9.0   \n",
       "12835392                          NaN                     NaN   \n",
       "28183298                          NaN                     NaN   \n",
       "19680294                          9.0                     9.0   \n",
       "\n",
       "          review_scores_value instant_bookable          cancellation_policy  \\\n",
       "id                                                                            \n",
       "30813194                  8.0                t                     flexible   \n",
       "2823607                   9.0                f                     flexible   \n",
       "12835392                  NaN                t                     flexible   \n",
       "28183298                  NaN                f  strict_14_with_grace_period   \n",
       "19680294                  9.0                t                     flexible   \n",
       "\n",
       "          reviews_per_month  \n",
       "id                           \n",
       "30813194               2.00  \n",
       "2823607                0.90  \n",
       "12835392               0.03  \n",
       "28183298                NaN  \n",
       "19680294               0.48  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models generally work best with numerical data only. Therefore, we will need to clean up the data.\n",
    "\n",
    "The first step of setting up our pre-processing pipeline will be identifying the various [`Transformer()`](https://scikit-learn.org/stable/data_transforms.html) classes that we will use. Transformers are Python classes that take `pandas` DataFrames / arrays of values as input, and output transformed arrays that may even have a different shape. \n",
    "\n",
    "The `sklearn` module provides many useful transformers, as well as the ability to create custom transformers. The transformer classes are of the form `Transformer(arguments)` and we will use the `fit(data)`, `transform(data)`, and `fit_transform()` methods to process our data.\n",
    "\n",
    "There are a few important requirements for a good machine learning dataset:\n",
    "\n",
    "1. Features should not have any missing values.<br><br>\n",
    "\n",
    "2. All features must be numerical.<br><br>\n",
    "\n",
    "3. Features should have the same or similar scales (in particular, range and expected value).\n",
    "\n",
    "Many machine learning models assume these requirements are met and will not perform well with data that does not adhere to them.\n",
    "\n",
    "Next, we will cover the main types of transformers necessary for our pipeline. Afterwards, we will show how to link them together to build a pipeline and apply them to our data features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputers\n",
    "\n",
    "The first assumption is that our data has no missing values. During exploration, we discovered this is not the case. There are a number of ways that we can work with missing data. In Part 1 of the module, we simply dropped records where values were missing. This strategy can work in certain circumstances, but we may find that we lose valuable data.\n",
    "\n",
    "Alternatively, we can fill in the missing data according to an appropriate rule. A variety of imputation rules exist. One option is to assign a placeholder value indicating missing data. For example, an \"other\" category. We could also fill in these values according to a measure of central tendency (mean, median, or mode). Or, in special cases, such as time series, we could interpolate values based on the values around them.\n",
    "\n",
    "For **imputation**, we will use the [`SimpleImputer()`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) transformer. This class offers a variety of options we can use for different types of features. For our categorical values, we can assign missing values to an \"other\" category &mdash; e.g. `SimpleImputer(strategy='constant', fill_value='Other')`. For our numerical values, we can use the \"median\" method (`SimpleImputer(strategy='median')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['House' 'Apartment' 'House' ... 'House' 'Apartment' 'Condominium']\n",
      "['House' 'Apartment' 'House' ... 'House' 'Apartment' 'Condominium']\n",
      "[10  2  1 ...  2  4  2]\n",
      "[10.  2.  1. ...  2.  4.  2.]\n"
     ]
    }
   ],
   "source": [
    "print(df[['property_type']].values[:, 0])\n",
    "tfmr = SimpleImputer(strategy='constant', fill_value='Other')\n",
    "# we place double brackets as a trick - sklearn expects arrays of the shape (n, 1). \n",
    "# the pandas Series.values method gives the shape (n, ), so we pass a DataFrame instead\n",
    "# by including a list of one column header.\n",
    "print(tfmr.fit_transform(df[['property_type']].values)[:, 0])\n",
    "\n",
    "print(df[['accommodates']].values[:, 0])\n",
    "tfmr = SimpleImputer(strategy='median')\n",
    "print(tfmr.fit_transform(df[['accommodates']].values)[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical encoders\n",
    "\n",
    "Since all of our data has to be numerical, we need a mechanism to transform our categorical data. For our categorical features, we can use **one-hot encoding** to represent the different categories numerically. With this approach, each unique value of a feature is given its own column. Thus, the data will be encoded with a \"1\" in the column which matches the value and a \"0\" in every other column.\n",
    "\n",
    "The use of multiple columns via one-hot encoding ensures that our categorical values are not treated as ordinal data. For example, if instead we used the values \"1\" and \"2\" to represent \"bed\" and \"floor\" in a single column, a model might consider these values as more similar than the values \"1\" and \"4\" representing \"bed\" and \"cot.\" We don't want to introduce additional associations between values through the choice of encoding scheme.\n",
    "\n",
    "We will use the [`OneHotEncoder(sparse=False)`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) transformer. We specify `sparse=False` to avoid outputting a sparse `scipy` matrix. A spare matrix can be extremely useful for handling very large datasets with mostly zero entries. However, for ease of understanding, we will stick to `numpy` arrays for this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['House']\n",
      " ['Apartment']\n",
      " ['House']\n",
      " ...\n",
      " ['House']\n",
      " ['Apartment']\n",
      " ['Condominium']]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(df[['property_type']].values)\n",
    "tfmr = OneHotEncoder(sparse=False)\n",
    "print(tfmr.fit_transform(df[['property_type']].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scalers\n",
    "Using different features that have hugely different scales can be problematic for some machine learning models. Therefore, it is good practice to implement a **scaling function** on numerical values. As an example, our `latitude` feature has a very small range and very high mean relative to a feature such as `accommodates`.\n",
    "\n",
    "There are two main scaling methods:\n",
    "\n",
    "1. **Normalization** or *min/max scaling* shifts and rescales data points between the values 0 and 1. This method is more likely to be affected by outliers.<br><br>\n",
    "\n",
    "2. **Standardization** works best with normally distributed data by subtracting the mean value from each data point and dividing by the standard deviation.\n",
    "\n",
    "There is no guaranteed rule for choosing between these methods, but data exploration can help to identify features that may be problematic for either method.\n",
    "\n",
    "We will use the [`MinMaxScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) on all of our numerical (discrete and continuous) features. Our exploration did not reveal many outliers and it is best to use a consistent scaling method. If you would like to experiment, you can replace the `MinMaxScaler()` with the [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and examine the impact on the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  2  1 ...  2  4  2]\n",
      "[0.6        0.06666667 0.         ... 0.06666667 0.2        0.06666667]\n",
      "[43.64616766 43.64105127 43.66724069 ... 43.67788972 43.66907246\n",
      " 43.63804609]\n",
      "[-0.69774523 -0.80533284 -0.25462118 ... -0.03069313 -0.21610266\n",
      " -0.8685259 ]\n"
     ]
    }
   ],
   "source": [
    "print(df[['accommodates']].values[:, 0])\n",
    "tfmr = MinMaxScaler()\n",
    "print(tfmr.fit_transform(df[['accommodates']].values)[:, 0])\n",
    "\n",
    "print(df[['latitude']].values[:, 0])\n",
    "tfmr = StandardScaler()\n",
    "print(tfmr.fit_transform(df[['latitude']].values)[:, 0])\n",
    "\n",
    "# NOTE: If you get a DataConversionWarning you can safely ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformers\n",
    "There are a great variety of transformers available in the `sklearn` package. However, if you can't find a transformer that suit your needs, it is straightforward to create your own! Any function can be easily wrapped in a tranformer class using the [`FunctionTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html).\n",
    "\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  2  1 ...  2  4  2]\n",
      "[11  3  2 ...  3  5  3]\n"
     ]
    }
   ],
   "source": [
    "def plusone(x):\n",
    "    return x + 1\n",
    "\n",
    "# print the original values\n",
    "print(df[['accommodates']].values[:, 0])\n",
    "\n",
    "# apply the transformation\n",
    "tfmr = FunctionTransformer(func=plusone, validate=False)\n",
    "\n",
    "# print the values plus one\n",
    "print(tfmr.fit_transform(df[['accommodates']].values)[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable buckets\n",
    "\n",
    "We should sort our features according to the different transformations each feature will need. In the code below, there are four different buckets.\n",
    "\n",
    "1. The categorical variables will need to be imputed and encoded.<br><br>\n",
    "\n",
    "2. The discrete variables will be imputed and scaled.<br><br>\n",
    "\n",
    "3. The continuous variables will also be imputed and scaled.<br><br>\n",
    "\n",
    "4. The dependent variable `price` will be imputed.\n",
    "\n",
    "**NOTE:** The dependent variable typically does not need to be scaled. And, for our purposes, avoiding scaling will make it easier to understand our models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from Part 1, our data types\n",
    "categorical_vars = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n",
    "            'is_location_exact', 'property_type', 'room_type', 'bed_type', \n",
    "            'instant_bookable', 'cancellation_policy']\n",
    "\n",
    "discrete_vars = ['host_listings_count', 'accommodates', 'bathrooms', 'bedrooms', 'beds',\n",
    "                 'guests_included', 'availability_365']\n",
    "\n",
    "continuous_vars = ['latitude', 'longitude']\n",
    "\n",
    "dep_var = ['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for `ColumnTransformer` pipelines generally includes these steps:\n",
    "\n",
    "1. **Transformers**: In this case, the ones we have already discussed.<br><br>\n",
    "\n",
    "2. **Pipelines**: These link together sequential transformers, making a chain of transformers that will each take in an input array or matrix and pass on the transformed output. The [`Pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) function takes as input a list of `(name, transformer)` tuples.<br><br>\n",
    "\n",
    "3. **ColumnTransformer**: This function will apply any number of pipelines to designated columns of a 2-dimensional data structure, making it suitable for working with `pandas` DataFrames. By indicating which pipelines apply to which columns, we can build a concise workflow that works with a variety of features that require different transformations. The [`ColumnTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) pipeline takes as input a list of `(name, pipeline, column_headers)` tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing the pipeline together\n",
    "\n",
    "Here are the transformation pipelines for the categorical and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical - impute, one hot encode\n",
    "cat_si_step = ('si', SimpleImputer(strategy='constant', fill_value='Other'))\n",
    "cat_ohe_step = ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "cat_steps = [cat_si_step, cat_ohe_step]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "cat_transformers = [('cat', cat_pipe, categorical_vars)]\n",
    "\n",
    "# Numerical - impute, scale\n",
    "num_si_step = ('si', SimpleImputer(strategy='median'))\n",
    "num_scl_step = ('scl', MinMaxScaler())\n",
    "num_steps = [num_si_step, num_scl_step]\n",
    "num_pipe = Pipeline(num_steps)\n",
    "num_transformers = [('num', num_pipe, discrete_vars + continuous_vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use our pipelines to transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=cat_transformers + num_transformers)\n",
    "ct.fit(df[categorical_vars + discrete_vars + continuous_vars])\n",
    "X = ct.transform(df[categorical_vars + discrete_vars + continuous_vars])\n",
    "# We know from our exploration that the dependent variable 'price' does not have any missing values. \n",
    "# It is also generally not necessary to apply transformations to normalize or scale\n",
    "# the dependent variable.\n",
    "y = df[['price']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating machine learning models, it is good practice to split your data into subsets: training data and testing data (we can also include a validation subset). This should be done randomly to avoid introducing bias. We fit the model on the training data and test its prediction strength on the testing data using our performance metric. This can help us identify two potential problems: overfitting or underfitting the model.\n",
    "\n",
    "1. **Overfitting** is when a model has been too finely tuned towards the training dataset. This issue can often arise in cases where there are relatively few datapoints compared to the number of features. The model may be finding many relationships that are not actually generalizable beyond the training dataset, giving it poor performance on a test dataset that it has not seen before.<br><br>\n",
    "\n",
    "2. **Underfitting** occurs when a model does not fit the training data well and is unable to find trends. Underfitting is more likely to occur in the case of a dataset that is too simple (few relevant independent variables) or when there is a mismatch between the data and the model chosen (e.g. a linear model for a non-linear relationship).\n",
    "\n",
    "Sklearn has the useful [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function for this purpose. We will split our independent $X$ values and dependent $y$ values each into a train set and a test set. A $\\text{20%}$ split is a typical parameter to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15717, 68) (3930, 68) (15717, 1) (3930, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling\n",
    "\n",
    "Random sampling is an effective way to avoid **sampling bias**, which occurs when certain groups in a dataset are over- or under-sampled. However, in small datasets random sampling can miss key structure in the data that is important to our research question, especially if we are interested in the tail ends of our dependent variable (e.g. the highest price AirBnB offerings). We can aim to preserve the overall distribution of data in our train and test datasets by using **stratified sampling**. \n",
    "\n",
    "For stratified sampling we segment our dataset into equally spaced **strata** and maintain the proportion of data in each stratum for our subsets.\n",
    "\n",
    "In Part 1 of the module, we saw that the `latitude` feature follows a somewhat bimodal distribution. Intuitively, we should also expect latitude to be an important feature &mdash; e.g. the farther south we are in Toronto, the closer to downtown. To ensure that this feature is sampled in a way that maintains its proportions, let's try stratified sampling.\n",
    "\n",
    "Sklearn offers the [`StratifiedShuffleSplit()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) class to make stratified sampling easy. We will use the `split()` method and provide our dataset and the chosen feature as arguments. The method will maintain even proportions across the chosen feature in the train and test datasets. We will first have to make a new column to create strata for our continuous latitude variable. In this case, we can simply multiply by 10 and take the ceiling (`[np.ceil()`](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ceil.html) to get five equally spaced and discrete strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437.0    3017\n",
      "438.0     838\n",
      "439.0      57\n",
      "436.0      17\n",
      "Name: lat_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['lat_cat'] = np.ceil(df['latitude']*10)\n",
    "print(df['lat_cat'].sample(frac=.2, random_state=0).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `split()` method. This will provide us with indices that we can use to select from our $X$ and $y$ data, even though we got the indices from our original `df`. Our output should show dimensions of four arrays. The first two will have the same number of columns (all of our independent variables) and the second two will have one column (our dependent variable). They will be split in a ratio of 4:1 (or 0.8:0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15717, 68) (3930, 68) (15717, 1) (3930, 1)\n"
     ]
    }
   ],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "# note that we pass the dataframe as arguments to the .split() method \n",
    "# rather than the numpy matrix X. This is done for easy comprehension\n",
    "# as the numpy matrix no longer has labelled columns. You can compare the\n",
    "# shapes of X and df to ensure we are getting the right rows.\n",
    "for train_index, test_index in split.split(df, df['lat_cat'].fillna(df['lat_cat'].median())):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "        \n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Regressions\n",
    "\n",
    "We are now ready to build some models! At this stage in the course, we will not focus on the details of each model. Different types of regression models and their strengths and weaknesses will be covered in a later module. However, it is good practice to try a few different models for comparison. Below we will implement a simple linear regression using `sklearn`'s [`LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) model. The widely used ordinary least squares (OLS) regression is a tried and true method for picking up relationships in data and using these relationships to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a simple function to print out our results, including our accuracy metric ($\\text{RMSE}$), and a sample of 5 predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 143061029153.86243\n",
      "Predicted 1-5: [ 50.0625  75.375  143.4375  55.3125  63.9375]\n",
      "Actual 1-5: [ 45.  38.  76.  69. 100.]\n"
     ]
    }
   ],
   "source": [
    "def display_results(model, X, y):\n",
    "    print(\"RMSE:\", rmse(model.predict(X), y))\n",
    "    print(\"Predicted 1-5:\", model.predict(X_test[0:5]))\n",
    "    print(\"Actual 1-5:\", y_test[0:5, 0])\n",
    "    \n",
    "display_results(reg, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Regressions\n",
    "\n",
    "Next, we will implement the more advanced ElasticNet regression model ([`ElasticNet()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)), as well as the stochastic gradient descent regressor ([`SGDRegressor()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)) and the random forest regressor ([`RandomForestRegressor()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)). Each of these methods takes a different approach towards regression. Check out the [sklearn generalized linear models](https://scikit-learn.org/stable/modules/linear_model.html) documentation for an overview of these models and many others. You can also try implementing one of the other models described on that page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet\n",
    "\n",
    "The ElasticNet model implements methods to help with assumptions made in OLS regression. You do not need to worry about the details for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 229.05360081625304\n",
      "Predicted 1-5: [ 97.60287359 107.13705813 161.25775042 111.17237008  95.65962275]\n",
      "Actual 1-5: [ 45.  38.  76.  69. 100.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enr = ElasticNet()\n",
    "enr.fit(X_train, np.ravel(y_train))\n",
    "display_results(enr, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ElasticNet performs better than the OLS regression, with a lower $\\text{RMSE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDRegressor\n",
    "\n",
    "Next, we will try the SGDRegressor, an old optimization method that has more recently become a standard component  of artificial neural networks (along with [backpropogation](https://en.wikipedia.org/wiki/Backpropagation)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 242.53454576719116\n",
      "Predicted 1-5: [ 62.36513558  79.96480622 142.69314099  66.5694013   58.8709175 ]\n",
      "Actual 1-5: [ 45.  38.  76.  69. 100.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "clf = SGDRegressor(tol=1e-3)\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "display_results(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGDRegressor performs only slightly better than OLS regression based on its $\\text{RMSE}$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeRegressor\n",
    "\n",
    "Decision tree models create \"branches\" at different feature values &mdash; e.g. a private home will lead to a prediction of a higher price. Decision trees and related models are valuable for ease of interpretation &mdash; the `sklearn` class makes it easy to determine which features were most important in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 307.7835377080111\n",
      "Predicted 1-5: [ 49.  84. 102.  76.  31.]\n",
      "Actual 1-5: [ 45.  38.  76.  69. 100.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(X_train, np.ravel(y_train))\n",
    "display_results(dtr, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some notable underfitting in the DecisionTreeRegressor based on its higher $\\text{RMSE}$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 4: Building a regression model\n",
    "\n",
    "Check out the list of regression models available in [`sklearn`](https://scikit-learn.org/stable/modules/linear_model.html). Try implementing one not listed above, and run it multiple times with some different parameters.\n",
    "\n",
    "If you are interested in understanding some of the improvements made in the ElasticNet model, try the [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) or [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) models.\n",
    "\n",
    "If you would like to improve on the DecisionTreeRegressor you could try the [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), an ensemble model that fits multiple decision tree regressors over subsets of the data. This is most effective when you have overfitting in your decision tree, but can still improve on $\\text{RMSE}$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282.00170703915944\n",
      "\n",
      "[ 58.6  35.3 115.7  80.1  50.3]\n",
      "[ 45.  38.  76.  69. 100.]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators=10)\n",
    "rfr.fit(X_train, np.ravel(y_train))\n",
    "print(rmse(rfr.predict(X), y))\n",
    "print()\n",
    "print(rfr.predict(X_test[0:5]))\n",
    "print(y_test[0:5, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "Splitting our data into a train set and a test set is a good start to avoiding overfitting. However, for a more comprehensive assessment of model performance we should cross-validate our models.\n",
    "\n",
    "**K-fold cross validation** is a method where we randomly split our data into $k$ subsets called **folds**. We then train our model $k$ times, each time picking one fold as our validation or test set and training on the remaining subsets. This method greatly reduces sampling bias and gives us an estimate not just of average model performance, but also the standard deviation of model performance. This extra information allows us to assess both the precision and accuracy of our model.\n",
    "\n",
    "We can use `sklearn`'s `cross_val_score()` function for easy cross-validation. Sklearn provides a number of different options for performance metrics. We will select `'neg_mean_squared_error'` and keep in mind that we will have to take the negative and square root of the result to get $\\text{RMSE}$. Remember that the lower the $\\text{RMSE}$ is, the better our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([129.36841654, 325.20647905, 251.16609114, 215.60607491,\n",
       "       128.32872966])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "enr = ElasticNet()\n",
    "scores = cross_val_score(enr, X, np.ravel(y), cv=5, scoring='neg_mean_squared_error')\n",
    "np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a simple function to print out our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [129.36841654 325.20647905 251.16609114 215.60607491 128.32872966]\n",
      "Mean: 209.9351582610659\n",
      "Standard deviation: 75.06028987182748\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", np.sqrt(-scores))\n",
    "    print(\"Mean:\", np.sqrt(-scores).mean())\n",
    "    print(\"Standard deviation:\", np.sqrt(-scores).std())\n",
    "\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [116.70389341 316.70472724 239.23929545 203.26493558 116.01687203]\n",
      "Mean: 198.38594474246673\n",
      "Standard deviation: 76.35280840436911\n"
     ]
    }
   ],
   "source": [
    "clf = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "scores = cross_val_score(clf, X, np.ravel(y), cv=5, scoring='neg_mean_squared_error')\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [267.65195109 338.96800347 390.61114641 226.2553639  270.12384017]\n",
      "Mean: 298.72206100898495\n",
      "Standard deviation: 58.477298256677486\n"
     ]
    }
   ],
   "source": [
    "rfr = DecisionTreeRegressor()\n",
    "scores = cross_val_score(dtr, X, np.ravel(y), cv=5, scoring='neg_mean_squared_error')\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [196.10592872 312.83027956 327.9051165  210.66556454 147.49146413]\n",
      "Mean: 238.99967069078767\n",
      "Standard deviation: 69.81630703598498\n"
     ]
    }
   ],
   "source": [
    "# You can add cross-validation for your model from exercise 4 here\n",
    "# Cross-validation\n",
    "rfr = RandomForestRegressor(n_estimators=10)\n",
    "scores = cross_val_score(rfr, X, np.ravel(y), cv=5, scoring='neg_mean_squared_error')\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our ElasticNet model still looks effective, we actually find that the SGD regressor has, on average, a lower $\\text{RMSE}$. Cross-validation can give you much more confidence in your calculation of performance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "\n",
    "Once we have chosen a model type, it is important to tune its **hyperparameters**. The hyperparameters are the parameters that are decided before beginning the training process. We have already seen some hyperparameters as arguments above (e.g. for SGDRegressor). We will discard the assumption that these default hyperparameters are the best possible options, and we will instead test different parameters to determine which works best.\n",
    "\n",
    "For our hyperparameter search we can use grid search or randomized search.\n",
    "\n",
    "* **Grid search** tries every possible combination from lists of different parameters and determines which combination is most effective.<br><br>\n",
    "\n",
    "* **Randomized search** will select random combinations of hyperparameters following defined rules or boundaries. This method is effective for very large search spaces, where trying out every combination is not computationally practical.\n",
    "\n",
    "For this module, we will use grid search. Sklearn's [`GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class implements this method alongside cross-validation for a convenient hyperparameter search. See below for implementing GridSearchCV with the SGDRegressor. Randomized search is also available with [`RandomizedSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). Finally, some models (e.g. ElasticNet) have more efficient hyperparameter search methods based on their mathematical characteristics (see [`ElasticNetCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV)).\n",
    "\n",
    "The output printed from this code simply describes the settings chosen for the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SGDRegressor(max_iter=10000),\n",
       "             param_grid={'alpha': array([1.e-04, 1.e-05, 1.e-06]),\n",
       "                         'learning_rate': ['invscaling'],\n",
       "                         'loss': ['squared_loss', 'huber',\n",
       "                                  'epsilon_insensitive'],\n",
       "                         'penalty': ['l2']},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# we create a dictionary of lists, each key is a parameter name\n",
    "# and the list is the possible values it can take\n",
    "# we have reduced the grid options to allow faster parameter searching\n",
    "# for a larger search you can include 'l1' and 'elasticnet' penalties\n",
    "# and 'constant' and 'optimal' learning_rates\n",
    "param_grid = {\n",
    "    'alpha': 10.0 ** -np.arange(4, 7),\n",
    "    'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],\n",
    "    'penalty': ['l2'],\n",
    "    'learning_rate': ['invscaling'],\n",
    "}\n",
    "# we can bump up the max_iterations parameter, otherwise some of our models will fail to converge\n",
    "clf = SGDRegressor(max_iter=10000, tol=1e-3)\n",
    "# we pass the model, our parameter grid, and cross-validation parameters to the class\n",
    "grid_search = GridSearchCV(estimator=clf, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           cv=5)\n",
    "# last, we fit our data. This will take a while...\n",
    "grid_search.fit(X=X, y=np.ravel(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at our `grid_search`'s `best_params_` and `best_estimator_` attributes to see which combination of hyperparameters was ultimately the most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1e-06, 'learning_rate': 'invscaling', 'loss': 'squared_loss', 'penalty': 'l2'}\n",
      "\n",
      " SGDRegressor(alpha=1e-06, max_iter=10000)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(\"\\n\",grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out the results of all of the models tested. Let's add a rule to filter down to only those which performed better than the default version we found originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = grid_search.cv_results_\n",
    "for mean_score, params in zip(cv_scores['mean_test_score'], cv_scores['params']):\n",
    "    if np.sqrt(-mean_score) < 171.9:\n",
    "        print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's grab our best model and do some performance testing. First, on our original test set, and then with our cross-validation methodology. Here the output will show the accuracy ($\\text{RMSE}$) of the best model, as well as a sample of five predicted and actual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 273.75464940398314\n",
      "Predicted 1-5: [ 56.34975551  77.89305493 141.35962025  65.4314188   57.40356336]\n",
      "Actual 1-5: [ 45.  38.  76.  69. 100.]\n"
     ]
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "display_results(final_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [115.91320689 316.52052753 239.06725939 203.89615179 117.16605072]\n",
      "Mean: 198.51263926555234\n",
      "Standard deviation: 76.20952015227081\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(final_model, X, np.ravel(y), cv=5, scoring='neg_mean_squared_error')\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: Launch, Monitor, Maintain\n",
    "\n",
    "Getting a model up an running is only the beginning of successful machine learning. For your model to be useful, it will likely need to be launched; set up for automated data ingestion and reporting; monitored for performance drops; and maintained or improved over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch\n",
    "\n",
    "First, always save any models you work on. Sklearn's [`joblib`](https://scikit-learn.org/stable/modules/model_persistence.html) provides methods to save models as a `.pkl` file (a standard python format for serializing data objects). Simply dump your model into a file, and load it back up when you need it or want to share it with a colleague."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dl/nzqz9d4s48z5l005svj0w2sr0000gn/T/ipykernel_31947/1337008730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from sklearn.externals import joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmy_model_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "joblib.dump(my_model, \"my_model.pkl\")\n",
    "my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is often new data available, writing code to automate the process of scraping new datasets and using them to further train your model will help it improve over time. Otherwise, performance can degrade over time as real word data and its underlying trends evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor\n",
    "\n",
    "If your model will be changing over time, it is essential to plan for regular testing. A wide range of scenarios can break your model, and you want to prepare for as many of them as possible. The introduction of new data features or removal of old features, changing data formats, and changes to the packages being used are just some examples of what could go wrong. Be sure to study up on version control and unit tests before deploying any machine learning model.\n",
    "\n",
    "You will also want to monitor for performance. Having automated systems in place to calculate and report on performance metrics will allow for faster resolution of issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintain\n",
    "\n",
    "Ultimately, you will need some level of human intervention to not only keep your model running smoothly, but also to help it improve over time. When tests fail or performance drops, there will be many possible causes, so it is important to know who is responsible for checking in on any problems and fixing them as they arise. Ideally, you can automate the learning process. If you do, be sure to save versions of your model at regular intervals so you can compare between different versions and roll back to an older version if necessary. Upload your code to a public or private repository like GitHub to easily track changes that you or others make over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Taking this further\n",
    "\n",
    "If you're interested in exploring ways to improve the models used here, don't hesitate to play around! You can download additional data from the AirBnB website linked below. Try choosing a different city or a different time period.\n",
    "\n",
    "You can also choose to include more features from the dataset. The `amenities` feature in particular contains a rich store of information, but in an inconvenient format. It would be excellent practice to take this feature, create a pre-processing pipeline that will turn it into usable numerical values, and include it in the models explored above. You will have to employ some Python string methods to parse the values contained in the feature, or you can use a text-based encoding method such as the `FeatureHasher()` or `CountVectorizer()`.\n",
    "\n",
    "A more advanced model would consider how the AirBnB landscape changes over time. There might be trends picked up in a year's worth of data that are not seen within any one month. There may also be other interesting features that aren't found directly in the data. This incomplete [kaggle kernel](https://www.kaggle.com/rudymizrahi/how-i-ranked-5th-160-in-deloitte-s-ml-competition) suggests calculating the distance of each location from the downtown core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Module**\n",
    "\n",
    "This notebook makes up one part of this module. Now that you have completed this part, please proceed to the next notebook in this module.\n",
    "\n",
    "If you have any questions, please reach out to your peers using the discussion boards. If you and your peers are unable to come to a suitable conclusion, do not hesitate to reach out to your instructor on the designated discussion board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators=10)\n",
    "rfr.fit(X_train, np.ravel(y_train))\n",
    "print(rmse(rfr.predict(X), y))\n",
    "print()\n",
    "print(rfr.predict(X_test[0:5]))\n",
    "print(y_test[0:5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "rfr = RandomForestRegressor(n_estimators=10)\n",
    "scores = cross_val_score(rfr, X, np.ravel(y), cv=5, scoring='neg_mean_squared_error')\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "AirBnB. (2019). Detailed Listings data for Toronto, January 2019. Retrieved from http://insideairbnb.com/get-the-data.html."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "496px",
    "width": "386px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
